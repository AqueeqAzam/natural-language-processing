{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCFAwsYEulr9BeEYVP0gQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AqueeqAzam/natural-language-processing/blob/main/nlp_with_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `NLP Pippeline:`\n",
        "\n",
        "Data Collection -> Text Cleaning -> Pre-Processing -> Feature Engineering ->\n",
        "Modling -> Evaluation -> Deplaoyment -> Updating"
      ],
      "metadata": {
        "id": "E7ne_w-8kyoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Text Pre-processing: `\n",
        "\n",
        "Text Cleaning( spelling checker, emoja handing, html tags), Basic Preprocessing(tokenization)"
      ],
      "metadata": {
        "id": "WXWiSVfaldRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Feature Engineering`\n",
        "\n",
        "One hot encoder, bag of word, n-gram, Tf-ldf, Word2vec"
      ],
      "metadata": {
        "id": "Cv3E7wbxl-i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Modelling:`\n",
        "\n",
        "Approaching to building model:\n",
        "\n",
        "ML Approach, DL Approach, Cloud API"
      ],
      "metadata": {
        "id": "9uuhuSbsmbz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Model Evaluation`\n",
        "\n",
        "Intrinsic evaluation, Extrinsic evaluation"
      ],
      "metadata": {
        "id": "-Qxwfr6mnDCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Elements of Text:`\n",
        "\n",
        "Hierarchy of Text -> No. words, No. aplhabates\n",
        "\n",
        "Tokens -> Seprate the word or sentence\n",
        "\n",
        "Vocabulary:\n",
        "\n",
        "Punctuation: ?, ., \"\", < e.t.c\n",
        "\n",
        "Root of word: pla, bas,\n",
        "\n",
        "Base of word: Play, Base\n",
        "\n"
      ],
      "metadata": {
        "id": "fvIuEzFfnXdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5nfYwMDj0Nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2663b6-4b67-4ec9-e0c6-5154461637b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " (',', ','),\n",
              " ('or', 'CC'),\n",
              " ('NLP', 'NNP'),\n",
              " (',', ','),\n",
              " ('combines', 'VBZ'),\n",
              " ('computational', 'JJ'),\n",
              " ('linguistics—rule-based', 'JJ'),\n",
              " ('modeling', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('human', 'JJ'),\n",
              " ('language—with', 'JJ'),\n",
              " ('statistical', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('machine', 'NN'),\n",
              " ('learning', 'NN'),\n",
              " ('models', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enable', 'VB'),\n",
              " ('computers', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('digital', 'JJ'),\n",
              " ('devices', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('recognize', 'VB'),\n",
              " (',', ','),\n",
              " ('understand', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('generate', 'NN'),\n",
              " ('text', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('speech', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "x = \"Natural language processing, or NLP, combines computational linguistics—rule-based modeling of human language—with statistical and machine learning models to enable computers and digital devices to recognize, understand and generate text and speech.\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# seperate the word\n",
        "w = word_tokenize(x)\n",
        "w\n",
        "\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# check part of speech\n",
        "p = pos_tag(w)\n",
        "p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Tokinization`"
      ],
      "metadata": {
        "id": "70d-5-sB3iqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = \"Natural language processing, or NLP, combines computational linguistics—rule-based modeling of human language—with statistical. While machine learning models to enable computers and digital devices to recognize, understand and generate text and speech.\"\n",
        "var\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "sent = sent_tokenize(var)\n",
        "sent\n",
        "\n",
        "# with for loop\n",
        "for i in sent:\n",
        "  print(i)\n",
        "  print()\n",
        "\n",
        "word = word_tokenize(var)\n",
        "word"
      ],
      "metadata": {
        "id": "pD-cmoCS3o9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Stop Word Removle`"
      ],
      "metadata": {
        "id": "nlmMbQx49l0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "punctuation\n",
        "\n"
      ],
      "metadata": {
        "id": "i_JuErEo9r65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Stemming`\n",
        "\n",
        "Stemming is a technique used to extact the base from of the words by removing affixes ffrom them.\n",
        "\n",
        "i.e charging ----stemming----> chang\n",
        "\n",
        "charged ----stemming----> chang\n",
        "\n",
        "charge ----stemming----> chang"
      ],
      "metadata": {
        "id": "yIt5xKGQfHaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer, RegexpStemmer, PorterStemmer, SnowballStemmer\n",
        "\n",
        "l = LancasterStemmer()\n",
        "r = RegexpStemmer('ing')\n",
        "p = PorterStemmer()\n",
        "s = SnowballStemmer(\"english\")\n",
        "\n",
        "l.stem('charging')\n",
        "\n",
        "r.stem('buying')\n",
        "\n",
        "p.stem('fucking')\n",
        "\n",
        "s.stem('cried')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "61hkPPxxf1HW",
        "outputId": "4552caab-055c-4dea-dcf8-182464369796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fuck'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Lemmatization`\n",
        "\n",
        "It is technique like stemming but we get valid word.\n",
        "\n",
        "studying -> study\n",
        "\n",
        "studies -> study\n",
        "\n",
        "study -> study"
      ],
      "metadata": {
        "id": "xCsXNC0qjJkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "w1 = WordNetLemmatizer()\n",
        "w1.lemmatize('mice')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "NS1vqEE8jQT7",
        "outputId": "0b81ca6d-27cd-4fe6-c5e3-ecb032e26fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mouse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `n-gram`\n",
        "\n",
        "It  refers to contiguous sequences of n words extracted from text for language processing and analysis."
      ],
      "metadata": {
        "id": "gXRTMGi-l9e2"
      }
    },
    {
      "source": [
        "x = \"I am Genius Aqueeq who have good knowlege about AI\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "w = word_tokenize(x)\n",
        "w\n",
        "\n",
        "from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures, BigramCollocationFinder, ngrams\n",
        "\n",
        "b = BigramCollocationFinder.from_words(w)\n",
        "\n",
        "b.ngram_fd\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCDyQYsQp6Ca",
        "outputId": "a0d25eba-72d7-42d0-ef2b-66d7e86f7cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zip at 0x78e878e3fa00>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Count Vectorizer`"
      ],
      "metadata": {
        "id": "Mv1bEAI1w_Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = ['My name is aqueeq azam', 'and I am AI engineer']\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'name': l})\n",
        "df\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "new_data = cv.fit_transform(df['name']).toarray()\n",
        "new_data\n",
        "\n",
        "cv.vocabulary_"
      ],
      "metadata": {
        "id": "lOOvgLHRxEKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Word Sense disambiguation`"
      ],
      "metadata": {
        "id": "RDetsxrr8Fm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample sentence\n",
        "context_sentence = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
        "\n",
        "# Ambiguous word (replace with your word)\n",
        "ambiguous_word = 'bank'\n",
        "\n",
        "# Part-of-speech (POS) tag (e.g., 'n' for noun)\n",
        "pos = 'n'\n",
        "\n",
        "# Perform WSD with Lesk algorithm\n",
        "sense = lesk(context_sentence, ambiguous_word, pos)\n",
        "\n",
        "# Print the most likely sense (Synset object)\n",
        "print(sense)\n"
      ],
      "metadata": {
        "id": "JA3tYX1s8Q2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca8887f-91f5-4867-d04b-922e45095c48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('savings_bank.n.02')\n"
          ]
        }
      ]
    }
  ]
}